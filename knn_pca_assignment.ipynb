{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **What is K-Nearest Neighbors (KNN) and how does it work?**\n",
        "KNN is a non-parametric, instance-based machine learning algorithm used for classification and regression. It works by finding the 'K' closest training samples (using a distance metric like Euclidean distance) to a new point and assigning the majority class (for classification) or averaging their values (for regression).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Difference between KNN Classification and Regression**\n",
        "- **KNN Classification**: Predicts the class with the majority vote from K-nearest neighbors.\n",
        "- **KNN Regression**: Predicts the average (or weighted average) value from K-nearest neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Role of the distance metric in KNN**\n",
        "The distance metric (e.g., Euclidean, Manhattan, Minkowski) determines how \"closeness\" is measured. It directly affects which points are considered neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Curse of Dimensionality in KNN**\n",
        "As dimensions increase, data points become sparse, making distance measures less meaningful. This affects KNN’s performance because neighbors become less distinguishable.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **How to choose the best value of K in KNN**\n",
        "Use techniques like **cross-validation** to find the K that minimizes error on validation data. Too small K = noisy, too large K = too smooth.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **What are KD Tree and Ball Tree in KNN**\n",
        "Data structures to speed up nearest neighbor searches:\n",
        "- **KD Tree**: Efficient for low-dimensional data.\n",
        "- **Ball Tree**: Better for higher dimensions or non-uniform data.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **When to use KD Tree vs Ball Tree**\n",
        "- **KD Tree**: Use when dimension < 20.\n",
        "- **Ball Tree**: Use for high-dimensional or non-axis-aligned data.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Disadvantages of KNN**\n",
        "- Slow prediction time.\n",
        "- Sensitive to irrelevant features and feature scaling.\n",
        "- Struggles with high-dimensional data.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **Effect of feature scaling on KNN**\n",
        "Essential! Since KNN uses distance metrics, unscaled features can dominate the computation. Use standardization or normalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **What is PCA (Principal Component Analysis)**\n",
        "PCA is a dimensionality reduction technique that transforms data to a new coordinate system, maximizing variance along principal components.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. **How does PCA work**\n",
        "1. Standardize data\n",
        "2. Compute covariance matrix\n",
        "3. Compute eigenvectors and eigenvalues\n",
        "4. Project data onto top components\n",
        "\n",
        "---\n",
        "\n",
        "### 12. **Geometric intuition of PCA**\n",
        "PCA finds new axes (principal components) along which variance in data is maximized, effectively rotating the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. **Feature Selection vs. Feature Extraction**\n",
        "- **Selection**: Choose a subset of original features.\n",
        "- **Extraction**: Create new features (e.g., PCA components).\n",
        "\n",
        "---\n",
        "\n",
        "### 14. **Eigenvalues and Eigenvectors in PCA**\n",
        "- **Eigenvectors**: Directions of new feature space (principal components).\n",
        "- **Eigenvalues**: Amount of variance each eigenvector explains.\n",
        "\n",
        "---\n",
        "\n",
        "### 15. **How to decide number of PCA components**\n",
        "- Use the **explained variance ratio**.\n",
        "- Use a **Scree Plot** or keep enough components to explain 95%+ of variance.\n",
        "\n",
        "---\n",
        "\n",
        "### 16. **Can PCA be used for classification?**\n",
        "Yes. PCA reduces dimensionality, which can improve classifier performance, but it doesn't directly perform classification.\n",
        "\n",
        "---\n",
        "\n",
        "### 17. **Limitations of PCA**\n",
        "- Assumes linear relationships.\n",
        "- Doesn’t preserve class separability.\n",
        "- Sensitive to outliers.\n",
        "\n",
        "---\n",
        "\n",
        "### 18. **How KNN and PCA complement each other**\n",
        "PCA reduces noise and dimensionality, improving KNN performance (especially in high-dimensional datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### 19. **How KNN handles missing values**\n",
        "It doesn't natively. You can use **KNN Imputer** (e.g., in sklearn) which replaces missing values using neighbors' values.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. **PCA vs. Linear Discriminant Analysis (LDA)**\n",
        "- **PCA**: Unsupervised, maximizes variance.\n",
        "- **LDA**: Supervised, maximizes class separability."
      ],
      "metadata": {
        "id": "uAWA4BTCJP0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN & PCA Solutions (Practical Answers)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris, make_regression, make_classification, load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, KNeighborsTransformer\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, classification_report, roc_auc_score, precision_score, recall_score, f1_score, roc_curve\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. KNN Classifier on Iris dataset\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"1. KNN Classifier Accuracy on Iris:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 2. KNN Regressor on synthetic dataset\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "knn_reg = KNeighborsRegressor()\n",
        "knn_reg.fit(X_train, y_train)\n",
        "y_pred = knn_reg.predict(X_test)\n",
        "print(\"2. KNN Regressor MSE:\", mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# 3. KNN Classifier with Euclidean and Manhattan distance\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsClassifier(metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    print(f\"3. Accuracy with {metric} distance:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# 4. KNN with different K values and decision boundaries (example for 2D data)\n",
        "X_vis, y_vis = make_classification(n_samples=200, n_features=2, n_redundant=0, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vis, y_vis, test_size=0.2)\n",
        "for k in [1, 3, 5, 7]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    plt.figure()\n",
        "    sns.scatterplot(x=X_test[:,0], y=X_test[:,1], hue=knn.predict(X_test), palette='coolwarm')\n",
        "    plt.title(f\"4. Decision boundary for k={k}\")\n",
        "    plt.show()\n",
        "\n",
        "# 5. Feature Scaling comparison\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "knn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n",
        "acc_unscaled = knn_unscaled.score(X_test, y_test)\n",
        "X_train_scaled, X_test_scaled = train_test_split(X_scaled, test_size=0.2)\n",
        "knn_scaled = KNeighborsClassifier().fit(X_train_scaled, y_train)\n",
        "acc_scaled = knn_scaled.score(X_test_scaled, y_test)\n",
        "print(\"5. Accuracy without scaling:\", acc_unscaled)\n",
        "print(\"   Accuracy with scaling:\", acc_scaled)\n",
        "\n",
        "# 6. PCA Explained Variance\n",
        "X_pca, _ = make_classification(n_samples=200, n_features=10)\n",
        "pca = PCA()\n",
        "pca.fit(X_pca)\n",
        "print(\"6. Explained variance ratio:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# 7. KNN with and without PCA\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y_vis, test_size=0.2)\n",
        "knn_orig = KNeighborsClassifier().fit(X_train, y_train)\n",
        "acc_orig = knn_orig.score(X_test, y_test)\n",
        "X_pca_reduced = PCA(n_components=2).fit_transform(X_pca)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca_reduced, y_vis, test_size=0.2)\n",
        "knn_pca = KNeighborsClassifier().fit(X_train, y_train)\n",
        "acc_pca = knn_pca.score(X_test, y_test)\n",
        "print(\"7. Accuracy without PCA:\", acc_orig)\n",
        "print(\"   Accuracy with PCA:\", acc_pca)\n",
        "\n",
        "# 8. GridSearchCV for KNN\n",
        "param_grid = {'n_neighbors': [3, 5, 7], 'metric': ['euclidean', 'manhattan']}\n",
        "gs = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
        "gs.fit(X_train, y_train)\n",
        "print(\"8. Best Params from GridSearch:\", gs.best_params_)\n",
        "\n",
        "# 9. Misclassified samples\n",
        "print(\"9. Misclassified samples:\", np.sum(knn_pca.predict(X_test) != y_test))\n",
        "\n",
        "# 10. Cumulative Explained Variance Plot\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.title(\"10. Cumulative Explained Variance\")\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Variance\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 11. Weights parameter in KNN\n",
        "for w in ['uniform', 'distance']:\n",
        "    knn = KNeighborsClassifier(weights=w)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"11. Accuracy with weights={w}:\", knn.score(X_test, y_test))\n",
        "\n",
        "# 12. Regressor K values effect\n",
        "for k in [1, 3, 5, 7, 9]:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"12. MSE for k={k}:\", mean_squared_error(y_test, knn.predict(X_test)))\n",
        "\n",
        "# 13. KNN Imputer\n",
        "X_missing = X.copy()\n",
        "X_missing[::10] = np.nan\n",
        "imputer = KNNImputer()\n",
        "X_filled = imputer.fit_transform(X_missing)\n",
        "print(\"13. Missing values imputed (example):\", np.isnan(X_filled).sum())\n",
        "\n",
        "# 14. PCA 2D projection\n",
        "X_proj = PCA(n_components=2).fit_transform(X)\n",
        "plt.scatter(X_proj[:,0], X_proj[:,1], c=y)\n",
        "plt.title(\"14. PCA 2D Projection\")\n",
        "plt.show()\n",
        "\n",
        "# 15. KD Tree vs Ball Tree performance\n",
        "for algo in ['kd_tree', 'ball_tree']:\n",
        "    knn = KNeighborsClassifier(algorithm=algo)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"15. Accuracy with {algo}:\", knn.score(X_test, y_test))\n",
        "\n",
        "# 16. PCA Scree Plot\n",
        "plt.plot(pca.explained_variance_)\n",
        "plt.title(\"16. Scree Plot\")\n",
        "plt.xlabel(\"Component\")\n",
        "plt.ylabel(\"Eigenvalue\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 17. Precision, Recall, F1-Score\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"17. Precision:\", precision_score(y_test, y_pred, average='macro'))\n",
        "print(\"    Recall:\", recall_score(y_test, y_pred, average='macro'))\n",
        "print(\"    F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "# 18. Accuracy with different PCA components\n",
        "for n in [1, 2, 5]:\n",
        "    X_reduced = PCA(n_components=n).fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2)\n",
        "    knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "    print(f\"18. Accuracy with {n} PCA components:\", knn.score(X_test, y_test))\n",
        "\n",
        "# 19. Leaf size comparison\n",
        "for leaf in [10, 20, 30, 40]:\n",
        "    knn = KNeighborsClassifier(leaf_size=leaf)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"19. Accuracy with leaf_size={leaf}:\", knn.score(X_test, y_test))\n",
        "\n",
        "# 20. Before/After PCA plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.title(\"Original Data\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_proj[:, 0], X_proj[:, 1], c=y)\n",
        "plt.title(\"After PCA\")\n",
        "plt.show()\n",
        "\n",
        "# 21. KNN on Wine Dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2)\n",
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(\"21. Classification Report on Wine:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 22. KNN Regressor with different metrics\n",
        "for metric in ['euclidean', 'manhattan']:\n",
        "    knn = KNeighborsRegressor(metric=metric)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"22. MSE with {metric}:\", mean_squared_error(y_test, knn.predict(X_test)))\n",
        "\n",
        "# 23. ROC-AUC Score (binary example)\n",
        "X_bin, y_bin = make_classification(n_classes=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bin, y_bin)\n",
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "print(\"23. ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n",
        "\n",
        "# 24. Variance captured by each component\n",
        "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
        "plt.title(\"24. Variance by Each PCA Component\")\n",
        "plt.show()\n",
        "\n",
        "# 25. Feature selection before training\n",
        "X_selected = X[:, :5]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2)\n",
        "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
        "print(\"25. Accuracy with Feature Selection:\", knn.score(X_test, y_test))\n",
        "\n",
        "# 26. PCA Reconstruction Error\n",
        "pca = PCA(n_components=5)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "recon_error = np.mean((X - X_reconstructed)**2)\n",
        "print(\"26. Reconstruction Error:\", recon_error)\n",
        "\n",
        "# 27. Decision Boundary Visualization (2D)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "x_min, x_max = X_train[:, 0].min(), X_train[:, 0].max()\n",
        "y_min, y_max = X_train[:, 1].min(), X_train[:, 1].max()\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
        "plt.title(\"27. KNN Decision Boundary\")\n",
        "plt.show()\n",
        "\n",
        "# 28. PCA variance effect\n",
        "for n in [2, 4, 6]:\n",
        "    pca = PCA(n_components=n)\n",
        "    X_reduced = pca.fit_transform(X)\n",
        "    print(f\"28. Variance with {n} components:\", np.sum(pca.explained_variance_ratio_))\n"
      ],
      "metadata": {
        "id": "C-OKRSapKVSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}